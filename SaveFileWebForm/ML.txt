语料库：  https://github.com/SophonPlus/ChineseNlpCorpus/tree/master

《Pre-training of Deep Bidirectional Transformers for Language Understanding》
https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html

https://blog.csdn.net/weixin_43479947/article/details/127885471?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169985239416800182781974%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169985239416800182781974&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-15-127885471-null-null.142^v96^control&utm_term=bert%E5%AE%9E%E7%8E%B0nlp%E8%AF%84%E8%AE%BA%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90&spm=1018.2226.3001.4187

1.	https://arxiv.org/pdf/1810.04805.pdf
2.	1906.04341.pdf (arxiv.org)
3.	News text classification based on Bidirectional Encoder Representation from Transformers | IEEE Conference Publication | IEEE Xplore
【深度学习】预训练语言模型-BERT_bert 预训练模型_DonngZH的博客-CSDN博客
【科研】BERT模型理论详解-CSDN博客
【自然语言处理 | BERT】BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding论文讲解_旅途中的宽~的博客-CSDN博客
BERT模型系列大全解读_bert 分类模型_我就算饿死也不做程序员的博客-CSDN博客
【精选】NLP中BERT模型详解_nlp bert_lichji2016的博客-CSDN博客

https://blog.csdn.net/weixin_44799217/article/details/115374101?ops_request_misc=&request_id=&biz_id=102&utm_term=%E8%8B%B1%E6%96%87%E7%89%88BERT%20mask%E5%9B%BE%E8%A7%A3&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-115374101.142^v96^control&spm=1018.2226.3001.4187